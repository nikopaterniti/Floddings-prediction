{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05-image-compression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z1KEBaefXAA2"},"source":["- Based on: https://arxiv.org/pdf/1708.00838v1.pdf\n","- Code: https://github.com/kunalrdeshmukh/End-to-end-compression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUbjZejwXE8S","executionInfo":{"elapsed":25358,"status":"ok","timestamp":1610228291972,"user":{"displayName":"Diego Calanzone","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQl0xsN1aGzFIsLt4pNGlQEMoZ6T8dphFIbVB4zQ=s64","userId":"03698100722566364493"},"user_tz":-60},"outputId":"18c88ead-e245-4890-e7f9-41a8662da3ec"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","%cd drive/My\\ Drive/SWE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/My Drive/SWE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KtJY_GsQXABE"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUgtD-BaXABF"},"source":["### Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AU9CVqxWf2qd","executionInfo":{"elapsed":28990,"status":"ok","timestamp":1610228295623,"user":{"displayName":"Diego Calanzone","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQl0xsN1aGzFIsLt4pNGlQEMoZ6T8dphFIbVB4zQ=s64","userId":"03698100722566364493"},"user_tz":-60},"outputId":"22d8a38d-63be-4e24-e653-e9a08a0bc1d6"},"source":["ls datasets/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mmini15002\u001b[0m/  mini.tar.xz  timesteps.npy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0WsYqzZfsAC","executionInfo":{"elapsed":33167,"status":"ok","timestamp":1610228299812,"user":{"displayName":"Diego Calanzone","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQl0xsN1aGzFIsLt4pNGlQEMoZ6T8dphFIbVB4zQ=s64","userId":"03698100722566364493"},"user_tz":-60},"outputId":"780da005-1f07-4149-d857-ae21c43bcb2d"},"source":["timesteps = np.load('datasets/timesteps.npy')\n","print(timesteps.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1000, 1, 300, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"niThn95jXABI"},"source":["### Network"]},{"cell_type":"code","metadata":{"id":"BD-xtY7ssxZP"},"source":["from copy import deepcopy\n","\n","old_timesteps = deepcopy(timesteps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7TYDZlrXABP"},"source":["CHANNELS = 1\n","HEIGHT = timesteps.shape[2]\n","WIDTH = timesteps.shape[2]\n","EPOCHS = 200\n","#BATCH_SIZE = 2\n","#n_batches = int(timesteps.shape[0]/BATCH_SIZE)\n","LOG_INTERVAL = int(timesteps.shape[0]/10)\n","TRAIN_SIZE = int(0.8 * timesteps.shape[0])\n","\n","#timesteps = old_timesteps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRB_7Exss62E"},"source":["# timesteps = (timesteps - np.min(timesteps)) / (np.max(timesteps) - np.min(timesteps))\n","def standard_tts(timesteps):\n","    \n","  train = timesteps[:TRAIN_SIZE]\n","  test = timesteps[TRAIN_SIZE:]\n","\n","  train_mean = np.mean(train)\n","  train_std = np.mean(train)\n","\n","  # Normalize\n","  train = (train - train_mean) / (train_std)\n","  test = (test - train_mean) / (train_std)\n","\n","  return train, test, train_mean, train_std\n","\n","def min_max_tts(timesteps):\n","    \n","  train = timesteps[:TRAIN_SIZE]\n","  test = timesteps[TRAIN_SIZE:]\n","\n","  train_max = np.max(train)\n","  train_min = np.min(train)\n","\n","  # Normalize\n","  train = (train - train_min) / (train_max - train_min)\n","  test = (test - train_min) / (train_max - train_min)\n","\n","  return train, test, train_max, train_min"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpdFbkG8xtrR"},"source":["train, test, train_max, train_min = min_max_tts(timesteps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7kgHQu9XABS"},"source":["class Interpolate(nn.Module):\n","    def __init__(self, size, mode):\n","        super(Interpolate, self).__init__()\n","        self.interp = nn.functional.interpolate\n","        self.size = size\n","        self.mode = mode\n","        \n","    def forward(self, x):\n","        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNGuol1JXABV"},"source":["class End_to_end(nn.Module):\n","  def __init__(self):\n","    super(End_to_end, self).__init__()\n","    \n","    # Encoder\n","    self.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=3, stride=1, padding=1)\n","    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0)\n","    self.bn1 = nn.BatchNorm2d(64, affine=False)\n","    self.conv3 = nn.Conv2d(64, CHANNELS, kernel_size=3, stride=1, padding=1)\n","    \n","    # Decoder\n","    self.interpolate = Interpolate(size=HEIGHT, mode='bilinear')\n","    self.deconv1 = nn.Conv2d(CHANNELS, 64, 3, stride=1, padding=1)\n","    self.deconv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n","    self.bn2 = nn.BatchNorm2d(64, affine=False)\n","    \n","    self.deconv_n = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n","    self.bn_n = nn.BatchNorm2d(64, affine=False)\n","\n","    \n","    self.deconv3 = nn.ConvTranspose2d(64, CHANNELS, 3, stride=1, padding=1)\n","    \n","    self.relu = nn.ReLU()\n","  \n","  def encode(self, x):\n","    out = self.relu(self.conv1(x))\n","    out = self.relu(self.conv2(out))\n","    out = self.bn1(out)\n","    return self.conv3(out)\n","    \n","  \n","  def reparameterize(self, mu, logvar):\n","    pass\n","  \n","  def decode(self, z):\n","    upscaled_image = self.interpolate(z)\n","    out = self.relu(self.deconv1(upscaled_image))\n","    out = self.relu(self.deconv2(out))\n","    out = self.bn2(out)\n","    for _ in range(5):\n","      out = self.relu(self.deconv_n(out))\n","      out = self.bn_n(out)\n","    out = self.deconv3(out)\n","    final = upscaled_image + out\n","    return final,out,upscaled_image\n","\n","    \n","  def forward(self, x):\n","    com_img = self.encode(x)\n","    final,out,upscaled_image = self.decode(com_img)\n","    return final, out, upscaled_image, com_img, x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7aA_87cXABV","executionInfo":{"elapsed":44033,"status":"ok","timestamp":1610228310738,"user":{"displayName":"Diego Calanzone","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQl0xsN1aGzFIsLt4pNGlQEMoZ6T8dphFIbVB4zQ=s64","userId":"03698100722566364493"},"user_tz":-60},"outputId":"c41f955e-fb1f-4de3-9d4e-cd62f56b3204"},"source":["CUDA = torch.cuda.is_available()\n","if CUDA:\n","  model = End_to_end().cuda()\n","else :\n","  model = End_to_end()\n","\n","print(\"GPU available ? \"+str(CUDA))\n","  \n","optimizer = optim.Adam(model.parameters(), lr=1e-2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU available ? True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5e85D19uXABW"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"HVlzbmmdXABZ"},"source":["def loss_function(final_img,residual_img,upscaled_img,com_img,orig_img):\n","    \n","    com_loss = nn.MSELoss(size_average=False)(orig_img, final_img)\n","    rec_loss = nn.MSELoss(size_average=False)(residual_img,orig_img-upscaled_img)\n","\n","    return com_loss + rec_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Ok0kGPhjlNqEZ0q8CxtNTKPt0v8tc5fq"},"id":"yA3kMCBPXABi","outputId":"a2936686-3f6d-49d1-8cab-18425ed2a1d4"},"source":["train_losses = []\n","test_losses = []\n","\n","for epoch in range(EPOCHS):\n","    x = 0\n","    \n","    # Shuffle to remove sequentiality bias\n","    np.random.shuffle(train)\n","\n","    train_loss = 0\n","\n","    # TODO: create batches and train the net with them  \n","    for b in range(timesteps.shape[0]):\n","\n","        model.train()\n","        train_loss = 0\n","\n","        optimizer.zero_grad()\n","        # data = torch.Tensor(train[x:x+BATCH_SIZE, :, :, :WIDTH])\n","        data = torch.Tensor(train[x, :, :, :WIDTH]).expand(1, -1, -1, -1)\n","            \n","        if CUDA:\n","          final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","        else :\n","          final, residual_img, upscaled_image, com_img, orig_im = model(data)\n","\n","        loss = loss_function(final, residual_img, upscaled_image, com_img, orig_im)\n","        \n","        loss.backward()\n","        \n","        train_loss += loss.item()\n","        \n","        if b % int(train.shape[0]/100) == 0 and b is not 0:\n","          # train loss\n","          train_losses.append(loss.item())\n","\n","          # test loss\n","          data = torch.Tensor(test[int(test.shape[0]/2), :, :, :WIDTH]).expand(1,1, -1, -1)          \n","          final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","          test_loss = loss_function(final, residual_img, upscaled_image, com_img, orig_im)\n","          test_losses.append(test_loss.item())\n","\n","          # plot\n","          plt.title(\"Epoch {}-{}\".format(epoch, b))\n","          plt.plot(train_losses, label=\"train\")\n","          plt.plot(test_losses, label=\"test\")\n","          plt.legend()\n","          plt.show()\n","\n","          #plt.matshow(orig_im.detach().cpu().numpy()[0, 0])\n","          plt.matshow(final.detach().cpu().numpy()[0, 0])\n","          plt.show()\n","        \n","        optimizer.step()\n","\n","    # -----\n","    train_losses = []\n","    test_losses = []\n","    print('====> Epoch: {} Batch {} Average loss: {:.4f}'.format(epoch, b, train_loss / timesteps.shape[0]))\n","\n","    # test predict and comparison\n","    data = torch.Tensor(test[int(test.shape[0]/2), :, :, :WIDTH]).expand(1, -1, -1, -1)\n","    final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","\n","    torch.save(model.state_dict(), \"datasets/model\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"6-sFWWrEXABj"},"source":["data = torch.Tensor(timesteps[600, :, :, :WIDTH]).expand(1,1, -1, -1)          \n","final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","plt.matshow(orig_im.cpu().detach().numpy()[0,0])\n","plt.matshow(final.cpu().detach().numpy()[0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5YeCIwjMp-u"},"source":[""],"execution_count":null,"outputs":[]}]}